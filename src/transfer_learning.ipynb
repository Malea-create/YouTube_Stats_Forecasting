{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.stats import wasserstein_distance\n",
    "import time\n",
    "from multiprocessing import Process, Queue\n",
    "\n",
    "'''\n",
    "\n",
    "Excerpt from the original confidential repository\n",
    "\n",
    "'''\n",
    "\n",
    "def execute_sds_and_ntm(training_set, source_domain_list, ntm_method):\n",
    "\n",
    "    '''\n",
    "    chooses the best fitting sourcedomains according to the wasserstein metrik and weights the sampels with the choosen ntm methode\n",
    "\n",
    "    :param training_set: dataframe including all domains/ column 'source_domain' contains the informations which source domain the sample comes from\n",
    "\n",
    "    :param source_domain_list: list of all 'source_domain' names \n",
    "    \n",
    "    :param ntm_method: method to calculate sample weights must be \"kmm\" or \"tradaboost\"\n",
    "\n",
    "    :return: training set and sample weights for model application \n",
    "    '''\n",
    "\n",
    "    # set target domain\n",
    "\n",
    "    df_tar = training_set[training_set['source_domain'] == \"target\"]\n",
    "\n",
    "    # get best fitting domains and calculate sample weights\n",
    "\n",
    "    if len(df_tar) != 0:\n",
    "                \n",
    "        ### get best fitting source domains ###\n",
    "        \n",
    "        was_list__source_domain = []\n",
    "\n",
    "        # calculate wasserstein distance for each market combination\n",
    "\n",
    "        for i in source_domain_list:\n",
    "            df_src = training_set[training_set['source_domain'] == i]\n",
    "            df_src.fillna(0, inplace=True)\n",
    "\n",
    "            if len(df_src) == 0:\n",
    "                was = np.nan\n",
    "            else: \n",
    "                was = wasserstein_distance(df_src[\"rv\"],df_tar[\"rv\"])\n",
    "\n",
    "            was_list__source_domain.append(was)\n",
    "    \n",
    "        was_list_len = []\n",
    "\n",
    "        # get lenth of each market\n",
    "\n",
    "        for i in source_domain_list:\n",
    "\n",
    "            df_src = training_set[training_set['source_domain'] == i]\n",
    "            df_src.fillna(0, inplace=True)\n",
    "\n",
    "            was_list_len.append(len(df_src))\n",
    "\n",
    "        # put results into a df and calculate weight\n",
    "\n",
    "        d = {'was': was_list__source_domain, 'len': was_list_len, 'source_domain': source_domain_list, 'weight': was_list__source_domain * (was_list_len-np.sum(was_list_len))*-1}\n",
    "        df_was_markets = pd.DataFrame(data=d)\n",
    "        df_was_markets = df_was_markets.sort_values(by=['weight'])\n",
    "        df_was_markets.fillna(0, inplace=True)\n",
    "        print(df_was_markets)\n",
    "\n",
    "        # get source domains with min weights distance to target market\n",
    "\n",
    "        df_src = pd.DataFrame()\n",
    "\n",
    "        another_market = 1\n",
    "        \n",
    "        while another_market <= 4 and df_was_markets.iloc[another_market]['weight'] < df_was_markets.iloc[1]['weight']*1.2:\n",
    "            df_src_2nd_market = training_set[training_set['source_domain'] == df_was_markets.iloc[another_market]['source_domain']]\n",
    "            df_src = pd.concat([df_src, df_src_2nd_market])\n",
    "            print(\"Another source domain was added: \"+ df_was_markets.iloc[another_market]['source_domain'])\n",
    "            another_market += 1\n",
    "        \n",
    "        # calculate weights for each sample and set market as source dataset\n",
    "        \n",
    "        training_set = pd.concat([df_src, df_tar]) # overwrite with additional source domains that fit target market\n",
    "        training_set.fillna(0, inplace=True)\n",
    "\n",
    "        if ntm_method == \"kmm\":\n",
    "\n",
    "            kmm_weighting(training_set)\n",
    "\n",
    "        elif ntm_method == \"tradaboost\":\n",
    "\n",
    "            beta = tradaboost_weighting(df_src, df_tar)\n",
    "\n",
    "        else:\n",
    "            print(\"Please choose a sample weighting algorithm\")\n",
    "\n",
    "    else:\n",
    "\n",
    "        # if target is 0 weights choosen standard weights\n",
    "\n",
    "        beta = np.array ( [1]*len(training_set) ) # all weights are the same\n",
    "\n",
    "    return training_set, beta\n",
    "\n",
    "def kmm_weighting(training_set):\n",
    "\n",
    "    '''\n",
    "    Calculate sample weights with KMM\n",
    "\n",
    "    :param training_set: dataframe including all target and source domain sampels\n",
    "\n",
    "    :return beta: sampel weight the length of src+tar\n",
    "    '''\n",
    "\n",
    "\n",
    "    try: \n",
    "        kmm_1 = kmm.KMM()\n",
    "\n",
    "        q = Queue()\n",
    "        \n",
    "        # Start process from multiprocesses\n",
    "\n",
    "        p = Process(target=kmm_1.fit, args=(training_set._get_numeric_data(), df_tar._get_numeric_data(), 1, q))\n",
    "        p.start()\n",
    "\n",
    "        # Wait 2 seconds for the process to test wrather 2 iterations need less than 2 seconds\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "        # Check if process is still running\n",
    "\n",
    "        if p.is_alive():\n",
    "\n",
    "            print (\"process is still running, it will be terminated now\")\n",
    "\n",
    "            # Terminate process\n",
    "\n",
    "            p.terminate()\n",
    "            p.join()\n",
    "\n",
    "            # set standard weights by using the cosine distance as backup\n",
    "\n",
    "            matrix = pairwise_distances(training_set._get_numeric_data(),df_tar._get_numeric_data(), metric='cosine')\n",
    "            df_matrix = pd.DataFrame(matrix)\n",
    "            df_matrix = df_matrix.mean(axis=1)\n",
    "            beta = df_matrix\n",
    "\n",
    "        else:\n",
    "\n",
    "            # get return value from whole process with 100 iterations\n",
    "\n",
    "            p = Process(target=kmm_1.fit, args=(training_set._get_numeric_data(), df_tar._get_numeric_data(), 100, q))\n",
    "            p.start()\n",
    "            beta = q.get()\n",
    "            beta = beta.clip(min=0).flatten()\n",
    "        \n",
    "    except MemoryError as err: \n",
    "        print(\"Error: \", err)\n",
    "        beta = np.array ( [1]*len(training_set) )\n",
    "        print(\"Beta is assigned\",beta)\n",
    "        pass\n",
    "\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Split the data \n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(['views','likes','dislikes','comment_count'], axis=1), df['likes'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Get weights\n",
    "execute_sds_and_ntm(training_set= , source_domain_list= , ntm_method= )\n",
    "\n",
    "# Train a decision tree regressor\n",
    "regressor = DecisionTreeRegressor()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = np.sqrt ( mean_squared_error(y_test, y_pred) )\n",
    "print('Root Mean Squared Error:', rmse)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
