{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "## DO NOT RUN THIS NOTEBOOK - Die Daten sind bereits verarbeitet gespeichert und auskommentieren würde den bisherigen Output überschreiben. Dieser kann helfen die Funktionen nachzuvollziehen.\n",
    "\n",
    "\n",
    "Dieses Notebook wendet ein Data Cleaning auf diese Daten an und unterteilt diese in länder- und jahresspezifische csv-files, die im Ordner \"Data/preprocessed_data\" gespeichert werden."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-02 14:21:18.838144: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/leamayer/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "\n",
    "# Preprocessing\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import nltk\n",
    "\n",
    "# Preprocess features\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "nltk.download('stopwords')\n",
    "from profanity_check import predict_prob\n",
    "\n",
    "# Preprocess targets\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enrichment of the Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Missing Values or Subscription Details\n",
    "\n",
    "Das Abonnieren eines Kanals auf YouTube kann darauf hinweisen, wie die anderen Videos dieses Youtubers abschneiden. Daher ist die Abonnentenzahl ein Maß für die Reichweite des YouTubers. [1] Je nach Reichweite des Publikums, das durch ein Video erreicht werden kann, ist die Abonnentenzahl ein wichtiges Merkmal bei der Vorhersage der potenziellen Aufrufe eines Videos. \n",
    "\n",
    "Im Folgenden wurde die `YouTube Data API v3` von `www.googleapis.com` verwendet, um die Abonnentenzahl des Kanals zum df hinzuzufügen. Darüber hinaus wurde versucht die API zu verwenden um Missing Values im Dataframe zu ersetzen. Durch ein tägliches Limit bei den Request konnte dieser Ansatz jedoch nicht weiter verfolgt werden.\n",
    "\n",
    "[1] https://tuberanker.com/blog/what-are-the-benefits-of-subscribing-to-a-youtube-channel (04.06.23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# The following youtube_authenticate function is available under this url: https://www.thepythoncode.com/article/using-youtube-api-in-python#google_vignette\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "\n",
    "import urllib.parse as p\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "SCOPES = [\"https://www.googleapis.com/auth/youtube.force-ssl\"]\n",
    "\n",
    "def youtube_authenticate():\n",
    "    os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n",
    "    api_service_name = \"youtube\"\n",
    "    api_version = \"v3\"\n",
    "    client_secrets_file = \"client_secret_883243427026-gmpvuoto8cd0gsl6djl80sesaa6b5jo0.apps.googleusercontent.com.json\"\n",
    "    creds = None\n",
    "    # the file token.pickle stores the user's access and refresh tokens, and is\n",
    "    # created automatically when the authorization flow completes for the first time\n",
    "    if os.path.exists(\"token.pickle\"):\n",
    "        with open(\"token.pickle\", \"rb\") as token:\n",
    "            creds = pickle.load(token)\n",
    "    # if there are no (valid) credentials availablle, let the user log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(client_secrets_file, SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        # save the credentials for the next run\n",
    "        with open(\"token.pickle\", \"wb\") as token:\n",
    "            pickle.dump(creds, token)\n",
    "\n",
    "    return build(api_service_name, api_version, credentials=creds)\n",
    "\n",
    "# authenticate to YouTube API\n",
    "youtube = youtube_authenticate()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# See the Youtube API Documentation for more insights: https://developers.google.com/youtube/v3/docs/subscriptions/list?hl=de&apix=true\n",
    "\n",
    "def get_channel_details(youtube, **kwargs):\n",
    "\n",
    "    request = youtube.channels().list(\n",
    "        part=\"statistics\", # just get stats\n",
    "        **kwargs\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    return response'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40881/40881 [22:59<00:00, 29.63it/s]\n"
     ]
    }
   ],
   "source": [
    "'''# get subscription count and save it to the df\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for i in tqdm(range (0, len(df))):\n",
    "\n",
    "    try:\n",
    "\n",
    "        # get current channel Name\n",
    "        channel_name = df.iloc[i][\"channel_title\"]\n",
    "\n",
    "        # make API call to get channel infos\n",
    "        response = get_channel_details(youtube, forUsername=channel_name)\n",
    "        items = response.get(\"items\")[0]\n",
    "        statistics = items[\"statistics\"]\n",
    "\n",
    "        # get stats infos\n",
    "        subscriberCount = statistics[\"subscriberCount\"]\n",
    "\n",
    "        # save subscriptions to df\n",
    "        df.at[i,\"subscriber_count\"] = int(subscriberCount)\n",
    "\n",
    "    except:\n",
    "\n",
    "        df.at[i,\"subscriber_count\"] = np.NAN'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "Im Folgenden werden die Daten verarbeitet, indem diese bereinigt und mit NLP-Preprocessing Methoden bearbeitet werden. Damit soll die Qualität, die Aussagekraft und die politische Korrektheit der Texte gewehrleistet werden. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Thumnails and add Path to jpg to the df\n",
    "\n",
    "Jedes YouTube-Video wird durch eine Thumbnail dargestellt, ein kleines Bild, das zusammen mit dem Titel und dem Kanal als \"Cover\" des Videos dient. Interessante und gut gestaltete Thumbnails ziehen Zuschauer an, während verwirrende und minderwertige Thumbnails die Zuschauer dazu bringen, woanders hinzuklicken. Ein Beweis dafür, wie wichtig ein guter Thumbnail ist: 90 % der erfolgreichsten YouTube-Videos haben Kunden-Thumbnails [2]. \n",
    "\n",
    "Dementsprechend kann und wurde diese Funktion verwendet, um die Thumbnails zu laden. Jedoch werden diese Daten als bereits genannten Gründen nicht für die finalen Modelle verwendet.\n",
    "\n",
    "[2] YouTube Creator Academy. Lesson: Make click- able thumbnails. https://creatoracademy.youtube.com/page/lesson/thumbnails#yt-creators-strategies-5. (04.06.23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:08<00:00, 58.34it/s]\n"
     ]
    }
   ],
   "source": [
    "'''# load thumbnail and save picture path in df\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def load_and_save_thumbnail(url, iterator):\n",
    "    try:\n",
    "\n",
    "        # get unique thumbnail id \n",
    "        splitted_words = url.split(\"/\")\n",
    "        thumbnail_id = splitted_words[4]\n",
    "\n",
    "        # create filename to save thumbnail\n",
    "        file_path = f\"../Data/thumbnails/{thumbnail_id}.jpg\"\n",
    "        #df_w_jgp.at[iterator,\"thumbnail_link\"] = thumbnail_id\n",
    "\n",
    "        # check if thumbnail is already downloaded\n",
    "        if os.path.exists(file_path):\n",
    "            pass\n",
    "        else:\n",
    "            # download and save thumbnail \n",
    "            urllib.request.urlretrieve(url, file_path)\n",
    "\n",
    "    except urllib.error.URLError as e:\n",
    "\n",
    "        df_w_jgp.at[iterator,\"thumbnail_link\"] = np.NAN\n",
    "        # print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "for i in tqdm(range (0, len(df))):\n",
    "\n",
    "    # get thumbnail url\n",
    "    url = df.iloc[i][\"thumbnail_link\"]\n",
    "\n",
    "    # safe thumnail and write path into df\n",
    "    load_and_save_thumbnail(url, i)  '''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(df):\n",
    "\n",
    "    # Drop Unnececary Columns\n",
    "\n",
    "    df = df [[\"channel_title\",\"tags\",\"description\", \"title\", \"views\", 'trending_date', 'video_id']]\n",
    "    \n",
    "    # Handling Missing Values\n",
    "\n",
    "    if df.isnull().any().any() or df.isna().any().any():\n",
    "\n",
    "        num = df.isnull().sum().sum() + df.isna().sum().sum() # Count Missing Values\n",
    "\n",
    "        print(\">>>\",num, \"Missing Values are beeing handeled\")\n",
    "\n",
    "        df = df.dropna() # Drop rows with missing values\n",
    "        #df.fillna(value) # Fill missing values with a specific value\n",
    "\n",
    "    else:\n",
    "        print(\">>> No Missing Values detected\")\n",
    "        \n",
    "    # Remove Duplicate\n",
    "\n",
    "    if df.duplicated().any():\n",
    "\n",
    "        num = df.duplicated().sum() # Count Duplicates\n",
    "\n",
    "        print(\">>>\",num, \"Duplicates are beeing handeled\")\n",
    "\n",
    "        df.drop_duplicates()\n",
    "\n",
    "    else:\n",
    "        print(\">>> No Duplicates detected\")\n",
    "\n",
    "    # Convert Data Types\n",
    "\n",
    "    #print( df.dtypes ) # check data types\n",
    "\n",
    "    # Convert 'trending_date' to year\n",
    "    df[\"trending_date\"] = df[\"trending_date\"].str[6:].astype(int) # save trending year\n",
    "    df = df.rename(columns={'trending_date': 'trending_year'})\n",
    "\n",
    "    for column in df:\n",
    "\n",
    "        # Check if column is of type bool\n",
    "        if df[column].dtype == 'bool':\n",
    "            df[column] = df[column].astype(int)\n",
    "\n",
    "    # Check preprocessed data\n",
    "\n",
    "    print(f\">>> Updated Data Types \\n {df.dtypes}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemmer and Stopword Removal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Stopwords\n",
    "\n",
    "Um Wörter derselben Wortfamilie auf eine gemeinsame Stamm zurückführen, wird im Folgenden ein ***Stemmer*** auf die Features angewendet. Im Gegensatz zu einem Lemmatisierer, der die Wörter auf ihre Grundform reduziert, ist der Stemmer wesentlich schneller, da nur die Wortendungen entfernt werden. Dies kann zu grammatikalisch inkonsistenten Sätzen führen, was aber bei dieser Aufgabe zu vernachlässigen ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add content speficic stopwords by select the top N words with the highest TF-IDF scores as potential stopwords\n",
    "\n",
    "def get_content_stopwords(texts):\n",
    "    \n",
    "    vectorizer = TfidfVectorizer() # TfidfVectorizer object from sklearn\n",
    "\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts) \n",
    "\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    avg_tfidf_scores = tfidf_matrix.mean(axis=0).tolist()[0] # Get average TF-IDF score for each word\n",
    "\n",
    "    word_scores = list(zip(feature_names, avg_tfidf_scores))\n",
    "\n",
    "    word_scores.sort(key=lambda x: x[1], reverse=True) # Sort the word_scores in descending order\n",
    "\n",
    "    N = 250 # Number of stopwords to select\n",
    "    stopwords_content = [word for word, score in word_scores[:N]] # Potential stopwords\n",
    "\n",
    "    print(f\">>> {len(stopwords_content)} content specific stopwords are identified\")\n",
    "\n",
    "    return stopwords_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stemmer and stopwords\n",
    "\n",
    "def get_stopwords(texts):\n",
    "\n",
    "    # Stopwords\n",
    "    stop_words_nlkt = set(stopwords.words('english'))\n",
    "\n",
    "    # add contect speficic stopwords \n",
    "    original_stopword_len = len(stop_words_nlkt)\n",
    "\n",
    "    stop_words = stop_words_nlkt.copy()\n",
    "\n",
    "    stop_words |= set(get_content_stopwords(texts)) # Add the additional wikipedia specific stopwords\n",
    "\n",
    "    merged_stopword_len = len(stop_words)\n",
    "\n",
    "    print(f\">>> {merged_stopword_len-original_stopword_len} additional stopwords have been added to the nlkt stopwords list\")\n",
    "\n",
    "    return stop_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Stemmer and Remove Stopwords from Text Features\n",
    "\n",
    "Um Wörter derselben Wortfamilie auf eine gemeinsame Stamm zurückführen, wird im Folgenden ein ***Stemmer*** auf die Features angewendet. Im Gegensatz zu einem Lemmatisierer, der die Wörter auf ihre Grundform reduziert, ist der Stemmer wesentlich schneller, da nur die Wortendungen entfernt werden. Dies kann zu grammatikalisch inkonsistenten Sätzen führen, was aber bei dieser Aufgabe zu vernachlässigen ist.\n",
    "\n",
    "Zusätzlich wird die erstellte ***Liste der Stopwords*** verwendet, um diese zu entfernen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Stemmer and Stopword removal\n",
    "\n",
    "# Get Stemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "feature_df_preprocessed = pd.DataFrame()\n",
    "\n",
    "def apply_stemmer_stopwords(df):\n",
    "\n",
    "    for feature in df:\n",
    "\n",
    "        print(f\"\\n--- Preprocessing {feature} feature ---\\n\")\n",
    "\n",
    "        texts = df[feature] #.to_numpy()\n",
    "\n",
    "        # Remove numbers and singel characters\n",
    "\n",
    "        texts_preprocessed = [' '.join([word for word in text.split() if not word.isnumeric()]) for text in texts] # Remove numbers\n",
    "        texts_preprocessed = [' '.join([word for word in text.split() if not len(word) > 1]) for text in texts] # Remove words with one character\n",
    "\n",
    "        print(\">>> Numbers and words with only one character are removed\")\n",
    "\n",
    "        # Get Stopwords\n",
    "        stop_words = get_stopwords(texts)\n",
    "\n",
    "        # Remove stop words from the text data and apply stemmer\n",
    "\n",
    "        texts_preprocessed = [' '.join([stemmer.stem(word) for word in text.split() if word.lower() not in stop_words]) for text in texts]\n",
    "\n",
    "        feature_df_preprocessed[feature] = texts_preprocessed\n",
    "\n",
    "        print(\">>> The Stemmer and the Stopword Removal are applied f.e.:\\n\")\n",
    "\n",
    "        print (texts[5]) # show example\n",
    "        print (texts_preprocessed[5])\n",
    "    \n",
    "    return feature_df_preprocessed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Profanity\n",
    "\n",
    "Damit das Modell nicht darauf trainiert wird, dass beleidigende Videoinhalte Views generieren können, wird an dieser Stelle ein Vortrainiertes Modell verwendet, um Videos mit solchen Inhalten zu identifizieren und zu entfernen. Mithilfe des Schwellenwertes kann die Stärke der Kontrolle abgepasst werden. Dieser wurde so festgelegt, dass versehentliche Aussortierungen möglichst verhindert werden können. Trotzdem gilt es, lieber Videos mit unfeindlichen Inhalten auszuschließen, als Videos mit feindlichen Inhalten zu behalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check text features for profanity and remove Videos with hate speech\n",
    "\n",
    "threshold = 0.9 # Strength of the algorithm punishment (Higher -> more profanity needed to exculde Video)\n",
    "\n",
    "def check_profanity(feature_df):\n",
    "\n",
    "    before = feature_df.shape[0]\n",
    "\n",
    "    for feature in feature_df:\n",
    "\n",
    "        print(f\"\\n--- Checking {feature} feature for profanity ---\\n\")\n",
    "\n",
    "        # for each row in dataframe column check if condition is true and save the index \n",
    "        # 0.0 -> no profanity / 1.0 -> profanity\n",
    "\n",
    "        rows_to_drop = [index for index, f in enumerate(feature_df[feature]) if predict_prob([f]) > threshold]\n",
    "\n",
    "        if len(rows_to_drop) >= 1:\n",
    "            \n",
    "            example = feature_df[feature][rows_to_drop[0]]\n",
    "            print(f\">>> Removed for example: {example}\")\n",
    "\n",
    "        try:\n",
    "            feature_df = feature_df.drop(rows_to_drop, axis = 0)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    after = feature_df.shape[0]\n",
    "\n",
    "    print(f\">>> Removed {before-after} Videos containing possible profanity\")\n",
    "\n",
    "    return feature_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Functions and save partial DataFrame as CSV\n",
    "\n",
    "Hier werden die definierten Preprocessing Funktionen angewendet. Die bearbeiteten Daten werden anschließend jahres- und länderspezifisch abgespeichert. (Hinweis: Ganzen Output anzeigen lassen um Funktion der einzelnen Methoden nachzuvollziehen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preprocessing CAvideos.csv ---\n",
      "\n",
      ">>> 2592 Missing Values are beeing handeled\n",
      ">>> No Duplicates detected\n",
      ">>> Updated Data Types \n",
      " channel_title    object\n",
      "tags             object\n",
      "description      object\n",
      "title            object\n",
      "views             int64\n",
      "trending_year     int64\n",
      "video_id         object\n",
      "dtype: object\n",
      "\n",
      "--- Checking channel_title feature for profanity ---\n",
      "\n",
      ">>> Removed for example: PowerfulJRE\n",
      "\n",
      "--- Checking tags feature for profanity ---\n",
      "\n",
      ">>> Removed for example: ryan|\"higa\"|\"higatv\"|\"nigahiga\"|\"i dare you\"|\"idy\"|\"rhpc\"|\"dares\"|\"no truth\"|\"comments\"|\"comedy\"|\"funny\"|\"stupid\"|\"fail\"\n",
      "\n",
      "--- Checking title feature for profanity ---\n",
      "\n",
      ">>> Removed for example: John Oliver video of Charlie Rose is extra creepy now\n",
      "\n",
      "--- Checking description feature for profanity ---\n",
      "\n",
      ">>> Removed for example: THIS VIDEO WILL MAKE YOU FORGET YOUR NAME ð®ðª\\nPatreon https://www.patreon.com/cowchop\\nSubscribe http://bit.ly/1RQtfNf Â \\nCow Chop Merch: http://bit.ly/2dY0HrO  \\nDiscuss: http://bit.ly/1qvrlLD Â \\nTwitter: https://twitter.com/CowChop Â \\nClick the link to sign up for Rooster Teeth FIRST and use the referral code COWCHOP https://goo.gl/NVZOT8\\n\\nYoutube has such a vast variety of great content! Why make Youtube great again when Youtube is already great?! See this video as an example. Where else can you go on an adventure through the deepest darkest weirdest side of video making and sharing, only here on this wonderful site will you get that experience, and now you can come along for the ride with us as we take you there!\\n\\nThe following is for those multicultural. \\n\\nHola gente. Es Trevor, tu narrador favorito aquÃ­ para darte otra gran historia que contar. Se trata de un pequeÃ±o amigo que conozco, un tipo pequeÃ±o muy especial que es extremadamente valiente y valiente. Todo comenzÃ³ cuando se liberÃ³ de su propio host, un intento de vivir por sÃ­ mismo. Para crear una vida y experimentar lo que todos buscamos, la felicidad. Lamentablemente, la pequeÃ±a etiqueta no pudo durar mucho, al menos no sin su host. Pero el caso es que el anfitriÃ³n no querÃ­a que le devolvieran la pequeÃ±a etiqueta. AsÃ­ que la etiqueta de la piel acaba de morir, gracias por sintonizar chicos.\\n\\nVideos\\n\\nFifty Shades Freed - https://youtu.be/nJCc5HRPxYA\\nJustin Bieber ignoring fan - https://youtu.be/BzwQczH4wU4\\nTanner Fox - https://youtu.be/ieUIoSzMDmY\\nReaction To Justin Bieber - https://youtu.be/DVTAZDta_Uo\\nIf you hate yourself watch this - https://youtu.be/O1GG2kOsAY8\\nThis video will make you forget your name - https://youtu.be/1xEHvFmFGxI\\n\\n\\nThank You ð®ðª\\nAFTER WATCHING THIS YOU WILL HATE COW CHOP â¢ WRONG SIDE OF YOUTUBE\n",
      ">>> Removed 8 Videos containing possible profanity\n",
      "\n",
      "--- Preprocessing USvideos.csv ---\n",
      "\n",
      ">>> 1140 Missing Values are beeing handeled\n",
      ">>> 48 Duplicates are beeing handeled\n",
      ">>> Updated Data Types \n",
      " channel_title    object\n",
      "tags             object\n",
      "description      object\n",
      "title            object\n",
      "views             int64\n",
      "trending_year     int64\n",
      "video_id         object\n",
      "dtype: object\n",
      "\n",
      "--- Checking channel_title feature for profanity ---\n",
      "\n",
      "\n",
      "--- Checking tags feature for profanity ---\n",
      "\n",
      ">>> Removed for example: ryan|\"higa\"|\"higatv\"|\"nigahiga\"|\"i dare you\"|\"idy\"|\"rhpc\"|\"dares\"|\"no truth\"|\"comments\"|\"comedy\"|\"funny\"|\"stupid\"|\"fail\"\n",
      "\n",
      "--- Checking title feature for profanity ---\n",
      "\n",
      ">>> Removed for example: When Someone Has A Crush on You | Lilly Singh\n",
      "\n",
      "--- Checking description feature for profanity ---\n",
      "\n",
      "\\nFashion, beauty tips, celebrity style, pop culture, videos, and moreâeverything you need to be ahead of the trends.  Fashion starts here.\\n\\nHillary Clinton On Why Sheâs Not Running For President Again | Teen Vogueear old activist and Harvard sophomore Nadya Okomoto who ran for city council in Cambridge, Massachusetts; 10-year old Mari Copeny who made headlines for drawing attention to the water crisis in her hometown of Flint, Michigan; 19-year-old Syrian refugee and advocate for girls' education Muzoon Almellehan; and 18-year-old model, artist, designer and transgender activist Hunter Schafer. \\n\\nStill havenât subscribed to Teen Vogue on YouTube? âºâº http://bit.ly/tvyoutubesub \n",
      ">>> Removed 39 Videos containing possible profanity\n",
      "\n",
      "--- Preprocessing GBvideos.csv ---\n",
      "\n",
      ">>> 1224 Missing Values are beeing handeled\n",
      ">>> 169 Duplicates are beeing handeled\n",
      ">>> Updated Data Types \n",
      " channel_title    object\n",
      "tags             object\n",
      "description      object\n",
      "title            object\n",
      "views             int64\n",
      "trending_year     int64\n",
      "video_id         object\n",
      "dtype: object\n",
      "\n",
      "--- Checking channel_title feature for profanity ---\n",
      "\n",
      ">>> Removed for example: Barcroft Animals\n",
      "\n",
      "--- Checking tags feature for profanity ---\n",
      "\n",
      ">>> Removed for example: christmas|\"john lewis christmas\"|\"john lewis\"|\"christmas ad\"|\"mozthemonster\"|\"christmas 2017\"|\"christmas ad 2017\"|\"john lewis christmas advert\"|\"moz\"\n",
      "\n",
      "--- Checking title feature for profanity ---\n",
      "\n",
      ">>> Removed for example: The Legend of Zelda: Breath of the Wild - Expansion Pass: DLC Pack 2 The Championsâ Ballad Trailer\n",
      "\n",
      "--- Checking description feature for profanity ---\n",
      "\n",
      ">>> Removed for example: This is my raw 100% raw opinion on Star Wars: The Last Jedi.\\nI live and bleed Star Wars, so this is just my viewpoint on the latest installment. Feel free to comment & we can constructively talk.\n",
      ">>> Removed 54 Videos containing possible profanity\n",
      "\n",
      "--- Preprocessing succesfull - Data has been saved to ../Data/processed_data/GBvideos_12.csv ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"../Data/original_data\"  # Path to original df\n",
    "result_folder_path = \"../Data/processed_data/\"  # Path for processed df\n",
    "iterator = 0\n",
    "\n",
    "if os.path.exists(result_folder_path):\n",
    "\n",
    "    # Iterate over files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "\n",
    "        if file_name.endswith('.csv'):  # Process only CSV files\n",
    "            \n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "            # Read the CSV file and create a DataFrame\n",
    "            df = pd.read_csv(file_path, encoding='latin-1')\n",
    "\n",
    "            print(f\"\\n--- Preprocessing {file_name} ---\\n\")\n",
    "\n",
    "            df = data_cleaning(df)\n",
    "\n",
    "            # --- Preprocess text features ---\n",
    "\n",
    "            df[[\"channel_title\",\"tags\",\"title\",\"description\"]] = check_profanity(df[[\"channel_title\",\"tags\",\"title\",\"description\"]]) # check for profanity and remove rows accordingly\n",
    "\n",
    "            #feature_df_preprocessed = apply_stemmer_stopwords(df[[\"channel_title\",\"tags\",\"title\",\"description\"]]) # remove stopwords and get stemm of words\n",
    "            #df[[\"channel_title\",\"tags\",\"title\",\"description\"]] = feature_df_preprocessed\n",
    "\n",
    "            # --- Group by trending year ---\n",
    "\n",
    "            original_dataset = file_name[:-4]\n",
    "            df_grouped = df.groupby('trending_year')\n",
    "\n",
    "            # --- Save partial dataframes ---\n",
    "\n",
    "            for group_name, group_data in df_grouped:\n",
    "\n",
    "                # Save final df as csv\n",
    "                csv_path = result_folder_path+original_dataset+\"_\"+str(group_name)+\".csv\"\n",
    "                group_data.to_csv(csv_path, index=False)\n",
    "\n",
    "    print(f\"\\n--- Preprocessing succesfull - Data has been saved to {csv_path} ---\\n\")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
