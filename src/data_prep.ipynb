{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "Wikipedia is an encyclopedia that covers a large amount of diverse topics. All articles are created, corrected and updated by individuals. The goal is to correctly document as many topics as possible by collecting the knowledge of a large number of people. However, some articles stand out due to their completeness, scope and presentation, and for this they are marked with the distinction of the Excellent Article. \n",
    "\n",
    "As part of the Natural Language Processing lecture, a classification of Wikipedia articles is to be carried out as a sub-task of an assignment with the goal of being able to identify excellent articles. This notebook contains the code to accomplish this goal and is structured as follows:\n",
    "\n",
    "- [1. Imports](article_classification.ipynb#1-imports)\n",
    "- [2. Enrichment of the Data](article_classification.ipynb#2-check-data-availability)\n",
    "\t- [2.1 Add Subscription Details](#thema1)\n",
    "\t- [2.2 Download Thumnails and add Path to jpg to the df](#thema2)\n",
    "- [3. Data Cleaning](article_classification.ipynb#3-data-processing)\n",
    "\t- [4.1 Define Data Cleaning](#thema1)\n",
    "\t- [4.2 Define String Encoding](#thema2)\n",
    "\t- [4.3 Apply Functions](#thema3)\n",
    "- [4. Export df]()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports\n",
    "Import the requiered libraties into the notebook.\n",
    "If some libraries are not installed, you can use the `requierements.txt` and run\n",
    "```\n",
    "$ pip install -r requirements.txt\n",
    "```\n",
    "in the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 14:22:06.589082: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Data/CAvideos.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enrichment of the Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Subscription Details for Channel Details\n",
    "\n",
    "Subscribing to a channel on YouTube can make it easier for you to watch multiple videos from the same channel. Therefore the Subscription count is a measure for the YouTubers reach. [1] According to range of the audiance that can be reached through a video, the subscription count is an important feature when forecasting the potential views of a video. \n",
    "\n",
    "In the following the `YouTube Data API v3` from `www.googleapis.com` is going to be used to add the subscription coun of the channel to the df.\n",
    "\n",
    "[1] https://tuberanker.com/blog/what-are-the-benefits-of-subscribing-to-a-youtube-channel (04.06.23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following youtube_authenticate function is available under this url: https://www.thepythoncode.com/article/using-youtube-api-in-python#google_vignette\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "\n",
    "import urllib.parse as p\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "SCOPES = [\"https://www.googleapis.com/auth/youtube.force-ssl\"]\n",
    "\n",
    "def youtube_authenticate():\n",
    "    os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n",
    "    api_service_name = \"youtube\"\n",
    "    api_version = \"v3\"\n",
    "    client_secrets_file = \"client_secret_883243427026-gmpvuoto8cd0gsl6djl80sesaa6b5jo0.apps.googleusercontent.com.json\"\n",
    "    creds = None\n",
    "    # the file token.pickle stores the user's access and refresh tokens, and is\n",
    "    # created automatically when the authorization flow completes for the first time\n",
    "    if os.path.exists(\"token.pickle\"):\n",
    "        with open(\"token.pickle\", \"rb\") as token:\n",
    "            creds = pickle.load(token)\n",
    "    # if there are no (valid) credentials availablle, let the user log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(client_secrets_file, SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        # save the credentials for the next run\n",
    "        with open(\"token.pickle\", \"wb\") as token:\n",
    "            pickle.dump(creds, token)\n",
    "\n",
    "    return build(api_service_name, api_version, credentials=creds)\n",
    "\n",
    "# authenticate to YouTube API\n",
    "youtube = youtube_authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the Youtube API Documentation for more insights: https://developers.google.com/youtube/v3/docs/subscriptions/list?hl=de&apix=true\n",
    "\n",
    "def get_channel_details(youtube, **kwargs):\n",
    "\n",
    "    request = youtube.channels().list(\n",
    "        part=\"statistics\", # just get stats\n",
    "        **kwargs\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40881/40881 [22:59<00:00, 29.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# get subscription count and save it to the df\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for i in tqdm(range (0, len(df))):\n",
    "\n",
    "    try:\n",
    "\n",
    "        # get current channel Name\n",
    "        channel_name = df.iloc[i][\"channel_title\"]\n",
    "\n",
    "        # make API call to get channel infos\n",
    "        response = get_channel_details(youtube, forUsername=channel_name)\n",
    "        items = response.get(\"items\")[0]\n",
    "        statistics = items[\"statistics\"]\n",
    "\n",
    "        # get stats infos\n",
    "        subscriberCount = statistics[\"subscriberCount\"]\n",
    "\n",
    "        # save subscriptions to df\n",
    "        df.at[i,\"subscriber_count\"] = int(subscriberCount)\n",
    "\n",
    "    except:\n",
    "\n",
    "        df.at[i,\"subscriber_count\"] = np.NAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Thumnails and add Path to jpg to the df\n",
    "\n",
    "Every YouTube video is represented by a thumbnail, a small image that, along with the title and channel, serves as the “cover” of the video. Thumbnails that are interest- ing and well-framed attract viewers, while those that are confusing and low-quality encourage viewers to click else- where. As a testament to the important of a good thumb- nail, 90% of the most successful YouTube videos have cus- tom thumbnails [2]. YouTube uploaders without the time or skills to create a custom thumbnail, however, must pick one of 3 frames automatically chosen from the video. Our mission is to improve this frame selection process and help uploaders select high quality frames that will attract viewers to their channel.\n",
    "\n",
    "http://cs231n.stanford.edu/reports/2017/pdfs/710.pdf\n",
    "[2] YouTube Creator Academy. Lesson: Make click- able thumbnails. https://creatoracademy. youtube.com/page/lesson/thumbnails# yt-creators-strategies-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:05<00:00, 19.27it/s]\n"
     ]
    }
   ],
   "source": [
    "# load thumbnail and save picture path in df\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "df_w_jgp = df.copy()\n",
    "\n",
    "def load_and_save_thumbnail(url, iterator):\n",
    "    try:\n",
    "\n",
    "        # get unique thumbnail id \n",
    "        splitted_words = url.split(\"/\")\n",
    "        thumbnail_id = splitted_words[4]\n",
    "\n",
    "        # create filename to save thumbnail\n",
    "        file_path = f\"../Data/thumbnails/{thumbnail_id}.jpg\"\n",
    "        df_w_jgp.at[iterator,\"thumbnail_link\"] = thumbnail_id\n",
    "\n",
    "        # check if thumbnail is already downloaded\n",
    "        if os.path.exists(file_path):\n",
    "            pass\n",
    "        else:\n",
    "            # download and save thumbnail \n",
    "            urllib.request.urlretrieve(url, file_path)\n",
    "\n",
    "    except urllib.error.URLError as e:\n",
    "\n",
    "        df_w_jgp.at[iterator,\"thumbnail_link\"] = np.NAN\n",
    "        # print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "for i in tqdm(range (0, len(df))):\n",
    "\n",
    "    # get thumbnail url\n",
    "    url = df.iloc[i][\"thumbnail_link\"]\n",
    "\n",
    "    # safe thumnail and write path into df\n",
    "    load_and_save_thumbnail(url, i)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(df):\n",
    "    \n",
    "    # Handling Missing Values\n",
    "\n",
    "    if df.isnull().any().any() or df.isna().any().any():\n",
    "\n",
    "        num = df.isnull().sum().sum() + df.isna().sum().sum() # Count Missing Values\n",
    "\n",
    "        print(\">>>\",num, \"Missing Values are beeing handeled\")\n",
    "\n",
    "        #df.dropna() # Drop rows with missing values\n",
    "        #df.fillna(value) # Fill missing values with a specific value\n",
    "\n",
    "    else:\n",
    "        print(\">>> No Missing Values detected\")\n",
    "        \n",
    "    # Remove Duplicate\n",
    "\n",
    "    if df.duplicated().any():\n",
    "\n",
    "        num = df.duplicated().sum() # Count Duplicates\n",
    "\n",
    "        print(\">>>\",num, \"Duplicates are beeing handeled\")\n",
    "\n",
    "        df.drop_duplicates()\n",
    "\n",
    "    else:\n",
    "        print(\">>> No Duplicates detected\")\n",
    "\n",
    "    # Handling Outliers\n",
    "\n",
    "\n",
    "    # Drop Unnececary Columns\n",
    "\n",
    "    df = df.drop(['video_id', 'trending_date', 'thumbnail_link', 'publish_time'], axis=1)\n",
    "\n",
    "    # Convert Data Types \n",
    "\n",
    "    # print( df.dtypes ) # check data types\n",
    "\n",
    "    # Convert 'date_column' to datetime\n",
    "    #df['trending_date'] = pd.to_datetime(df['trending_date'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "    # Convert 'timestamp_column' to timestamp\n",
    "    #df['publish_time'] = pd.to_datetime(df['publish_time'], infer_datetime_format=True)\n",
    "\n",
    "    for column in df:\n",
    "\n",
    "        # Check if column is of type bool\n",
    "        if df[column].dtype == 'bool':\n",
    "            df[column] = df[column].astype(int)\n",
    "\n",
    "    # Check preprocessed data\n",
    "\n",
    "    print(\">>> Updated Data Types \\n\", df.dtypes)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define String Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding(df_cont):# encode remaining objects\n",
    "\n",
    "    tokenizer = Tokenizer(\n",
    "        num_words=10000,\n",
    "        filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "    )\n",
    "\n",
    "    for column in df:\n",
    "\n",
    "        # Check if column is of type object\n",
    "        if df[column].dtype == 'object':\n",
    "\n",
    "            df[column] = df[column].astype(str)\n",
    "\n",
    "            print(f\">>> {column} is of type object and will be encoded\")\n",
    "\n",
    "            tokenizer.fit_on_texts(df[column])\n",
    "\n",
    "            # Get the word index\n",
    "            word_index = tokenizer.word_index\n",
    "\n",
    "            # Convert texts to sequences\n",
    "            sequences = tokenizer.texts_to_sequences(df[column])\n",
    "\n",
    "            df_cont[column] = sequences\n",
    "\n",
    "            df_cont\n",
    "\n",
    "            # split sequences into seperate columns\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    for column in df_cont:\n",
    "\n",
    "            # Check if column is of type object\n",
    "            if df_cont[column].dtype == 'object':\n",
    "\n",
    "                # Create separate columns for each element in the list\n",
    "                df_expanded = pd.DataFrame(df_cont[column].to_list(), columns=[f\"{column}_{i+1}\" for i in range(df_cont[column].str.len().max())])\n",
    "                \n",
    "                # Concatenate expanded columns with the original DataFrame\n",
    "                df_cont = pd.concat([df_cont.drop(column, axis=1), df_expanded], axis=1)\n",
    "                \n",
    "    return df_cont\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Functions and save final DataFrame as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/dataset_w_jpg.csv\n",
      ">>> 29236 Missing Values are beeing handeled\n",
      ">>> 221 Duplicates are beeing handeled\n",
      ">>> Updated Data Types \n",
      " title                      object\n",
      "channel_title              object\n",
      "category_id               float64\n",
      "tags                       object\n",
      "views                     float64\n",
      "likes                     float64\n",
      "dislikes                  float64\n",
      "comment_count             float64\n",
      "comments_disabled          object\n",
      "ratings_disabled           object\n",
      "video_error_or_removed     object\n",
      "description                object\n",
      "dtype: object\n",
      ">>> video_id is of type object and will be encoded\n",
      ">>> trending_date is of type object and will be encoded\n",
      ">>> title is of type object and will be encoded\n",
      ">>> channel_title is of type object and will be encoded\n",
      ">>> publish_time is of type object and will be encoded\n",
      ">>> tags is of type object and will be encoded\n",
      ">>> thumbnail_link is of type object and will be encoded\n",
      ">>> comments_disabled is of type object and will be encoded\n",
      ">>> ratings_disabled is of type object and will be encoded\n",
      ">>> video_error_or_removed is of type object and will be encoded\n",
      ">>> description is of type object and will be encoded\n",
      "../Data/CAvideos.csv\n",
      ">>> 2592 Missing Values are beeing handeled\n",
      ">>> No Duplicates detected\n",
      ">>> Updated Data Types \n",
      " title                     object\n",
      "channel_title             object\n",
      "category_id                int64\n",
      "tags                      object\n",
      "views                      int64\n",
      "likes                      int64\n",
      "dislikes                   int64\n",
      "comment_count              int64\n",
      "comments_disabled          int64\n",
      "ratings_disabled           int64\n",
      "video_error_or_removed     int64\n",
      "description               object\n",
      "dtype: object\n",
      ">>> video_id is of type object and will be encoded\n",
      ">>> trending_date is of type object and will be encoded\n",
      ">>> title is of type object and will be encoded\n",
      ">>> channel_title is of type object and will be encoded\n",
      ">>> publish_time is of type object and will be encoded\n",
      ">>> tags is of type object and will be encoded\n",
      ">>> thumbnail_link is of type object and will be encoded\n",
      ">>> description is of type object and will be encoded\n",
      "../Data/USvideos.csv\n",
      ">>> 1140 Missing Values are beeing handeled\n",
      ">>> 48 Duplicates are beeing handeled\n",
      ">>> Updated Data Types \n",
      " title                     object\n",
      "channel_title             object\n",
      "category_id                int64\n",
      "tags                      object\n",
      "views                      int64\n",
      "likes                      int64\n",
      "dislikes                   int64\n",
      "comment_count              int64\n",
      "comments_disabled          int64\n",
      "ratings_disabled           int64\n",
      "video_error_or_removed     int64\n",
      "description               object\n",
      "dtype: object\n",
      ">>> video_id is of type object and will be encoded\n",
      ">>> trending_date is of type object and will be encoded\n",
      ">>> title is of type object and will be encoded\n",
      ">>> channel_title is of type object and will be encoded\n",
      ">>> publish_time is of type object and will be encoded\n",
      ">>> tags is of type object and will be encoded\n",
      ">>> thumbnail_link is of type object and will be encoded\n",
      ">>> description is of type object and will be encoded\n",
      "../Data/GBvideos.csv\n",
      ">>> 1224 Missing Values are beeing handeled\n",
      ">>> 171 Duplicates are beeing handeled\n",
      ">>> Updated Data Types \n",
      " title                     object\n",
      "channel_title             object\n",
      "category_id                int64\n",
      "tags                      object\n",
      "views                      int64\n",
      "likes                      int64\n",
      "dislikes                   int64\n",
      "comment_count              int64\n",
      "comments_disabled          int64\n",
      "ratings_disabled           int64\n",
      "video_error_or_removed     int64\n",
      "description               object\n",
      "dtype: object\n",
      ">>> video_id is of type object and will be encoded\n",
      ">>> trending_date is of type object and will be encoded\n",
      ">>> title is of type object and will be encoded\n",
      ">>> channel_title is of type object and will be encoded\n",
      ">>> publish_time is of type object and will be encoded\n",
      ">>> tags is of type object and will be encoded\n",
      ">>> thumbnail_link is of type object and will be encoded\n",
      ">>> description is of type object and will be encoded\n",
      "        category_id       views      likes  dislikes  comment_count  title_1   \n",
      "0              10.0  17158579.0   787425.0   43420.0       125882.0    764.0  \\\n",
      "1              23.0   1014651.0   127794.0    1688.0        13030.0   6061.0   \n",
      "2              23.0   3191434.0   146035.0    5339.0         8181.0    743.0   \n",
      "3              24.0   2095828.0   132239.0    1989.0        17518.0     52.0   \n",
      "4              10.0  33523622.0  1634130.0   21082.0        85067.0    549.0   \n",
      "...             ...         ...        ...       ...            ...      ...   \n",
      "161954         10.0  25066952.0   268088.0   12783.0         9933.0    739.0   \n",
      "161955         10.0   1492219.0    61998.0   13781.0        24330.0   1871.0   \n",
      "161956         10.0  29641412.0   394830.0    8892.0        19988.0    652.0   \n",
      "161957         24.0  14317515.0   151870.0   45875.0        26766.0   1867.0   \n",
      "161958         10.0    607552.0    18271.0     274.0         1423.0    579.0   \n",
      "\n",
      "        title_2  title_3  title_4  title_5  ...  thumbnail_link_7  country   \n",
      "0         661.0     18.0    437.0    124.0  ...               NaN        0  \\\n",
      "1         225.0    718.0    647.0   2093.0  ...               NaN        0   \n",
      "2        1764.0    777.0   1402.0    465.0  ...               NaN        0   \n",
      "3        2159.0     45.0    609.0   5450.0  ...               NaN        0   \n",
      "4         662.0    338.0     22.0     59.0  ...               NaN        0   \n",
      "...         ...      ...      ...      ...  ...               ...      ...   \n",
      "161954    740.0    963.0     18.0    333.0  ...               7.0        3   \n",
      "161955   1872.0     74.0     21.0     63.0  ...               7.0        3   \n",
      "161956    709.0   1598.0      7.0     10.0  ...            3908.0        3   \n",
      "161957   1868.0   1869.0   1870.0     57.0  ...               7.0        3   \n",
      "161958     45.0     55.0    655.0   5597.0  ...               8.0        3   \n",
      "\n",
      "        comments_disabled  ratings_disabled  video_error_or_removed   \n",
      "0                     NaN               NaN                     NaN  \\\n",
      "1                     NaN               NaN                     NaN   \n",
      "2                     NaN               NaN                     NaN   \n",
      "3                     NaN               NaN                     NaN   \n",
      "4                     NaN               NaN                     NaN   \n",
      "...                   ...               ...                     ...   \n",
      "161954                0.0               0.0                     0.0   \n",
      "161955                0.0               0.0                     0.0   \n",
      "161956                0.0               0.0                     0.0   \n",
      "161957                0.0               0.0                     0.0   \n",
      "161958                0.0               0.0                     0.0   \n",
      "\n",
      "        description_915  thumbnail_link_8  thumbnail_link_9   \n",
      "0                   NaN               NaN               NaN  \\\n",
      "1                   NaN               NaN               NaN   \n",
      "2                   NaN               NaN               NaN   \n",
      "3                   NaN               NaN               NaN   \n",
      "4                   NaN               NaN               NaN   \n",
      "...                 ...               ...               ...   \n",
      "161954              NaN               8.0               NaN   \n",
      "161955              NaN               8.0               NaN   \n",
      "161956              NaN               7.0               8.0   \n",
      "161957              NaN               8.0               NaN   \n",
      "161958              NaN               NaN               NaN   \n",
      "\n",
      "        thumbnail_link_10  thumbnail_link_11  \n",
      "0                     NaN                NaN  \n",
      "1                     NaN                NaN  \n",
      "2                     NaN                NaN  \n",
      "3                     NaN                NaN  \n",
      "4                     NaN                NaN  \n",
      "...                   ...                ...  \n",
      "161954                NaN                NaN  \n",
      "161955                NaN                NaN  \n",
      "161956                NaN                NaN  \n",
      "161957                NaN                NaN  \n",
      "161958                NaN                NaN  \n",
      "\n",
      "[161959 rows x 1156 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "folder_path = \"../Data\"  # Replace with the path to your folder\n",
    "final_df = []  # List to store the DataFrames\n",
    "\n",
    "iterator = 0\n",
    "# Iterate over files in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.csv') and file_name != 'final_dataset.csv':  # Process only CSV files\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        print(file_path)\n",
    "\n",
    "        # Read the CSV file and create a DataFrame\n",
    "        df = pd.read_csv(file_path, encoding='latin-1')\n",
    "\n",
    "        cleaned_df = data_cleaning(df)\n",
    "\n",
    "        encoded_df = encoding(cleaned_df)\n",
    "\n",
    "        encoded_df[\"country\"] = iterator # file_name[:3]\n",
    "\n",
    "        iterator += 1\n",
    "\n",
    "        # Add the DataFrame to the list\n",
    "        final_df.append(encoded_df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "combined_df = pd.concat(final_df, ignore_index=True)\n",
    "\n",
    "# Print the combined DataFrame\n",
    "print(combined_df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export final df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values with 0\n",
    "#df_w_jgp= df_w_jgp.fillna(0)\n",
    "\n",
    "df_cnn = df_w_jgp[['views', 'thumbnail_link']]\n",
    "\n",
    "# Save final df as csv\n",
    "csv_path = \"../Data/final_dataset_cnn.csv\"\n",
    "df_cnn.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values with 0\n",
    "combined_df= combined_df.fillna(0)\n",
    "\n",
    "# Save final df as csv\n",
    "csv_path = \"../Data/final_dataset.csv\"\n",
    "combined_df.to_csv(csv_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
