{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "Wikipedia is an encyclopedia that covers a large amount of diverse topics. All articles are created, corrected and updated by individuals. The goal is to correctly document as many topics as possible by collecting the knowledge of a large number of people. However, some articles stand out due to their completeness, scope and presentation, and for this they are marked with the distinction of the Excellent Article. \n",
    "\n",
    "As part of the Natural Language Processing lecture, a classification of Wikipedia articles is to be carried out as a sub-task of an assignment with the goal of being able to identify excellent articles. This notebook contains the code to accomplish this goal and is structured as follows:\n",
    "\n",
    "- [1. Imports](article_classification.ipynb#1-imports)\n",
    "- [2. Enrichment of the Data](article_classification.ipynb#2-check-data-availability)\n",
    "\t- [2.1 Add Subscription Details](#thema1)\n",
    "\t- [2.2 Download Thumnails and add Path to jpg to the df](#thema2)\n",
    "- [3. Data Cleaning](article_classification.ipynb#3-data-processing)\n",
    "\t- [4.1 Define Data Cleaning](#thema1)\n",
    "\t- [4.2 Define String Encoding](#thema2)\n",
    "\t- [4.3 Apply Functions](#thema3)\n",
    "- [4. Export df]()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports\n",
    "Import the requiered libraties into the notebook.\n",
    "If some libraries are not installed, you can use the `requierements.txt` and run\n",
    "```\n",
    "$ pip install -r requirements.txt\n",
    "```\n",
    "in the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-01 10:35:43.615334: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enrichment of the Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Subscription Details for Channel Details\n",
    "\n",
    "Subscribing to a channel on YouTube can make it easier for you to watch multiple videos from the same channel. Therefore the Subscription count is a measure for the YouTubers reach. [1] According to range of the audiance that can be reached through a video, the subscription count is an important feature when forecasting the potential views of a video. \n",
    "\n",
    "In the following the `YouTube Data API v3` from `www.googleapis.com` is going to be used to add the subscription coun of the channel to the df.\n",
    "\n",
    "[1] https://tuberanker.com/blog/what-are-the-benefits-of-subscribing-to-a-youtube-channel (04.06.23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following youtube_authenticate function is available under this url: https://www.thepythoncode.com/article/using-youtube-api-in-python#google_vignette\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "\n",
    "import urllib.parse as p\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "SCOPES = [\"https://www.googleapis.com/auth/youtube.force-ssl\"]\n",
    "\n",
    "def youtube_authenticate():\n",
    "    os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n",
    "    api_service_name = \"youtube\"\n",
    "    api_version = \"v3\"\n",
    "    client_secrets_file = \"client_secret_883243427026-gmpvuoto8cd0gsl6djl80sesaa6b5jo0.apps.googleusercontent.com.json\"\n",
    "    creds = None\n",
    "    # the file token.pickle stores the user's access and refresh tokens, and is\n",
    "    # created automatically when the authorization flow completes for the first time\n",
    "    if os.path.exists(\"token.pickle\"):\n",
    "        with open(\"token.pickle\", \"rb\") as token:\n",
    "            creds = pickle.load(token)\n",
    "    # if there are no (valid) credentials availablle, let the user log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(client_secrets_file, SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        # save the credentials for the next run\n",
    "        with open(\"token.pickle\", \"wb\") as token:\n",
    "            pickle.dump(creds, token)\n",
    "\n",
    "    return build(api_service_name, api_version, credentials=creds)\n",
    "\n",
    "# authenticate to YouTube API\n",
    "youtube = youtube_authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the Youtube API Documentation for more insights: https://developers.google.com/youtube/v3/docs/subscriptions/list?hl=de&apix=true\n",
    "\n",
    "def get_channel_details(youtube, **kwargs):\n",
    "\n",
    "    request = youtube.channels().list(\n",
    "        part=\"statistics\", # just get stats\n",
    "        **kwargs\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40881/40881 [22:59<00:00, 29.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# get subscription count and save it to the df\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for i in tqdm(range (0, len(df))):\n",
    "\n",
    "    try:\n",
    "\n",
    "        # get current channel Name\n",
    "        channel_name = df.iloc[i][\"channel_title\"]\n",
    "\n",
    "        # make API call to get channel infos\n",
    "        response = get_channel_details(youtube, forUsername=channel_name)\n",
    "        items = response.get(\"items\")[0]\n",
    "        statistics = items[\"statistics\"]\n",
    "\n",
    "        # get stats infos\n",
    "        subscriberCount = statistics[\"subscriberCount\"]\n",
    "\n",
    "        # save subscriptions to df\n",
    "        df.at[i,\"subscriber_count\"] = int(subscriberCount)\n",
    "\n",
    "    except:\n",
    "\n",
    "        df.at[i,\"subscriber_count\"] = np.NAN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Thumnails and add Path to jpg to the df\n",
    "\n",
    "Every YouTube video is represented by a thumbnail, a small image that, along with the title and channel, serves as the “cover” of the video. Thumbnails that are interest- ing and well-framed attract viewers, while those that are confusing and low-quality encourage viewers to click else- where. As a testament to the important of a good thumb- nail, 90% of the most successful YouTube videos have cus- tom thumbnails [2]. YouTube uploaders without the time or skills to create a custom thumbnail, however, must pick one of 3 frames automatically chosen from the video. Our mission is to improve this frame selection process and help uploaders select high quality frames that will attract viewers to their channel.\n",
    "\n",
    "http://cs231n.stanford.edu/reports/2017/pdfs/710.pdf\n",
    "[2] YouTube Creator Academy. Lesson: Make click- able thumbnails. https://creatoracademy. youtube.com/page/lesson/thumbnails# yt-creators-strategies-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:08<00:00, 58.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# load thumbnail and save picture path in df\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "df_w_jgp = df.copy()\n",
    "\n",
    "def load_and_save_thumbnail(url, iterator):\n",
    "    try:\n",
    "\n",
    "        # get unique thumbnail id \n",
    "        splitted_words = url.split(\"/\")\n",
    "        thumbnail_id = splitted_words[4]\n",
    "\n",
    "        # create filename to save thumbnail\n",
    "        file_path = f\"../Data/thumbnails/{thumbnail_id}.jpg\"\n",
    "        #df_w_jgp.at[iterator,\"thumbnail_link\"] = thumbnail_id\n",
    "\n",
    "        # check if thumbnail is already downloaded\n",
    "        if os.path.exists(file_path):\n",
    "            pass\n",
    "        else:\n",
    "            # download and save thumbnail \n",
    "            urllib.request.urlretrieve(url, file_path)\n",
    "\n",
    "    except urllib.error.URLError as e:\n",
    "\n",
    "        df_w_jgp.at[iterator,\"thumbnail_link\"] = np.NAN\n",
    "        # print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "for i in tqdm(range (0, len(df))):\n",
    "\n",
    "    # get thumbnail url\n",
    "    url = df.iloc[i][\"thumbnail_link\"]\n",
    "\n",
    "    # safe thumnail and write path into df\n",
    "    load_and_save_thumbnail(url, i)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(df):\n",
    "\n",
    "    # Drop Unnececary Columns\n",
    "\n",
    "    df = df [[\"channel_title\",\"tags\",\"description\", \"title\", \"views\", 'trending_date']]\n",
    "    \n",
    "    # Handling Missing Values\n",
    "\n",
    "    if df.isnull().any().any() or df.isna().any().any():\n",
    "\n",
    "        num = df.isnull().sum().sum() + df.isna().sum().sum() # Count Missing Values\n",
    "\n",
    "        print(\">>>\",num, \"Missing Values are beeing handeled\")\n",
    "\n",
    "        #df.dropna() # Drop rows with missing values\n",
    "        #df.fillna(value) # Fill missing values with a specific value\n",
    "\n",
    "    else:\n",
    "        print(\">>> No Missing Values detected\")\n",
    "        \n",
    "    # Remove Duplicate\n",
    "\n",
    "    if df.duplicated().any():\n",
    "\n",
    "        num = df.duplicated().sum() # Count Duplicates\n",
    "\n",
    "        print(\">>>\",num, \"Duplicates are beeing handeled\")\n",
    "\n",
    "        df.drop_duplicates()\n",
    "\n",
    "    else:\n",
    "        print(\">>> No Duplicates detected\")\n",
    "\n",
    "    # Convert Data Types\n",
    "\n",
    "    #print( df.dtypes ) # check data types\n",
    "\n",
    "    # Convert 'trending_date' to year\n",
    "    df[\"trending_date\"] = df[\"trending_date\"].str[6:].astype(int) # save trending year\n",
    "    df = df.rename(columns={'trending_date': 'trending_year'})\n",
    "\n",
    "    for column in df:\n",
    "\n",
    "        # Check if column is of type bool\n",
    "        if df[column].dtype == 'bool':\n",
    "            df[column] = df[column].astype(int)\n",
    "\n",
    "    # Check preprocessed data\n",
    "\n",
    "    print(\">>> Updated Data Types \\n\", df.dtypes)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Functions and save partial DataFrame as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preprocessing CAvideos.csv ---\n",
      "\n",
      ">>> 2592 Missing Values are beeing handeled\n",
      ">>> No Duplicates detected\n",
      ">>> Updated Data Types \n",
      " channel_title    object\n",
      "tags             object\n",
      "description      object\n",
      "title            object\n",
      "views             int64\n",
      "trending_year     int64\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2j/wfkfpv3d0399hnl84gpblcp40000gn/T/ipykernel_99369/30329259.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"trending_date\"] = df[\"trending_date\"].str[6:].astype(int) # save trending year\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preprocessing USvideos.csv ---\n",
      "\n",
      ">>> 1140 Missing Values are beeing handeled\n",
      ">>> 49 Duplicates are beeing handeled\n",
      ">>> Updated Data Types \n",
      " channel_title    object\n",
      "tags             object\n",
      "description      object\n",
      "title            object\n",
      "views             int64\n",
      "trending_year     int64\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2j/wfkfpv3d0399hnl84gpblcp40000gn/T/ipykernel_99369/30329259.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"trending_date\"] = df[\"trending_date\"].str[6:].astype(int) # save trending year\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preprocessing GBvideos.csv ---\n",
      "\n",
      ">>> 1224 Missing Values are beeing handeled\n",
      ">>> 173 Duplicates are beeing handeled\n",
      ">>> Updated Data Types \n",
      " channel_title    object\n",
      "tags             object\n",
      "description      object\n",
      "title            object\n",
      "views             int64\n",
      "trending_year     int64\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2j/wfkfpv3d0399hnl84gpblcp40000gn/T/ipykernel_99369/30329259.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"trending_date\"] = df[\"trending_date\"].str[6:].astype(int) # save trending year\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "folder_path = \"../Data/original_data\"  # Path to original df\n",
    "result_folder_path = \"../Data/processed_data/\"  # Path for processed df\n",
    "iterator = 0\n",
    "\n",
    "# Iterate over files in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "\n",
    "    if file_name.endswith('.csv'):  # Process only CSV files\n",
    "        \n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        # Read the CSV file and create a DataFrame\n",
    "        df = pd.read_csv(file_path, encoding='latin-1')\n",
    "\n",
    "        print(f\"\\n--- Preprocessing {file_name} ---\\n\")\n",
    "\n",
    "        cleaned_df = data_cleaning(df)\n",
    "\n",
    "        original_dataset = file_name[:-4]\n",
    "\n",
    "        df_grouped = cleaned_df.groupby('trending_year')\n",
    "\n",
    "        for group_name, group_data in df_grouped:\n",
    "\n",
    "            # Save final df as csv\n",
    "            csv_path = result_folder_path+original_dataset+\"_\"+str(group_name)+\".csv\"\n",
    "            group_data.to_csv(csv_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
